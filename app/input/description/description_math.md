#### ニューラルネットワークの処理の流れ
下記は、ニューラルネットワークの順伝播と逆伝播のプロセス、そしてパラメータの更新の流れを表す数式の具体例です。<br>

1. パラメータの初期化

重みとバイアスを次のように定義します：<br>

$$
W^{(1)} = \begin{bmatrix} 0.1 & 0.2 \\ 0.3 & 0.4 \end{bmatrix}, \quad b^{(1)} = \begin{bmatrix} 0.01 \\ 0.02 \end{bmatrix}
$$

$$
W^{(2)} = \begin{bmatrix} 0.5 & 0.6 \end{bmatrix}, \quad b^{(2)} = 0.03
$$

2. 入力と真のラベルの定義

入力ベクトルと真のラベル（期待する値）を次のように定義します：<br>

$$
x = \begin{bmatrix} 0.7 \\ 0.8 \end{bmatrix}, \quad y_{\text{true}} = 1
$$

3. 順伝播

隠れ層と出力層を次のように計算します：<br>

$$
隠れ層: h = \tanh(W^{(1)}x + b^{(1)})
$$

$$
出力層: y_{\text{pred}} = \sigma(W^{(2)}h + b^{(2)})
$$

ここで、$ \sigma $ はシグモイド関数です。<br>

4. 損失の計算

クロスエントロピー損失を次のように計算します：<br>

$$
L = -y_{\text{true}} \log(y_{\text{pred}}) - (1 - y_{\text{true}}) \log(1 - y_{\text{pred}})
$$

5. 勾配の計算

損失関数の勾配を次のように計算します：<br>

$$
\frac{\partial L}{\partial W^{(1)}} = (W^{(2)} (y_{\text{pred}} - y_{\text{true}})) \cdot (1 - h^2) \cdot x^T
$$

$$
\frac{\partial L}{\partial b^{(1)}} = (W^{(2)} (y_{\text{pred}} - y_{\text{true}})) \cdot (1 - h^2)
$$

$$
\frac{\partial L}{\partial W^{(2)}} = (y_{\text{pred}} - y_{\text{true}}) \cdot h
$$

$$
\frac{\partial L}{\partial b^{(2)}} = y_{\text{pred}} - y_{\text{true}}
$$

6. パラメータの更新
パラメータの更新は次のように行います：<br>

$$
W^{(1)}_{\text{new}} = W^{(1)} - \eta \frac{\partial L}{\partial W^{(1)}}
$$

$$
b^{(1)}_{\text{new}} = b^{(1)} - \eta \frac{\partial L}{\partial b^{(1)}}
$$

$$
W^{(2)}_{\text{new}} = W^{(2)} - \eta \frac{\partial L}{\partial W^{(2)}}
$$

$$
b^{(2)}_{\text{new}} = b^{(2)} - \eta \frac{\partial L}{\partial b^{(2)}}
$$

ここで、$ \eta $ は学習率です。<br>
#### T5での学習の流れ
以下の図は、T5モデルがタスクを理解し、重みを更新する流れを示しています。<br>
![T5 Model Flow](https://showme.redstarplugin.com/d/q9aPYPlK)
1. 入力値をトークンに変換します。<br>
これは一般的には単語やサブワードのトークン化によって行われます。<br>
具体的な数式はありませんが、トークン化の結果、入力テキスト "Translate to Japanese: Thank you" はトークンのシーケンス ["Translate", "to", "Japanese", ":", "Thank", "you"] に変換されます。<br>

2. トークンをエンコーダーに渡します。<br>
エンコーダーはトークンをベクトル表現に変換します。<br>
この変換は埋め込み行列 $ E $ を使用して行われ、各トークン $ t_i $ のベクトル表現 $ v_i $ は $ v_i = E[t_i] $ と計算されます。<br>

3. エンコーダーは、トークンのベクトル表現に基づいて隠れ状態を計算します。<br>これは自己注意メカニズムとフィードフォワードネットワークを使用して行われます。<br>
自己注意メカニズムは以下の数式に基づいています：<br>

   $$ Attention(Q, K, V) = softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V $$

   ここで、$ Q $、$ K $、$ V $ はクエリ、キー、値の行列であり、これらはエンコーダーの入力トークンから計算されます。<br>
   また、$ d_k $ はキーの次元数です。<br>

4. エンコーダーの出力（隠れ状態）はデコーダーに渡されます。<br>

5. デコーダーはエンコーダーの出力と自分自身の過去の出力に基づいて次のトークンを予測します。<br>
これはエンコーダーの自己注意メカニズムと同じ数式を使用しますが、クエリはデコーダーから、キーと値はエンコーダーから取得します。<br>

6. デコーダーの出力はソフトマックス関数を通じて確率分布に変換されます。<br>
これは各トークンの予測確率を表します。<br>
ソフトマックス関数は以下の数式に基づいています：<br>

   $$ \hat{y}_i = \frac{exp(z_i)}{\sum_j exp(z_j)} $$

   ここで、$ z_i $ はデコーダーの出力（ロジット）、$ \hat{y}_i $ は予測確率分布です。<br>

7. 予測確率分布と真のラベルの確率分布との間のクロスエントロピー損失を計算します。<br>
クロスエントロピー損失は以下の数式に基づいて計算されます：<br>

   $$ L = -\sum_{i=1}^{n} y_i \log(\hat{y}_i) $$

   ここで、$ y_i $ は真のラベルの確率分布（one-hotエンコーディングされたラベル）、$ \hat{y}_i $ は予測確率分布、$ n $ はボキャブラリのサイズ（すなわち、可能なトークンの数）です。<br>

8. クロスエントロピー損失の勾配を計算し、これを使用してモデルのパラメータ（エンコーダーとデコーダーの重み）を更新します。<br>
これはバックプロパゲーションアルゴリズムと勾配降下法（またはその派生形）を使用して行われます。<br>
具体的な数式は以下の通りです：<br>

   $$ w_{new} = w_{old} - \alpha \frac{\partial L}{\partial w} $$

   ここで、$ w_{old} $ は更新前の重み、$ w_{new} $ は更新後の重み、$ \alpha $ は学習率（ステップサイズ）、$ \frac{\partial L}{\partial w} $ は損失関数の重みに対する勾配です。<br>

以上が、T5モデルが特定のタスク（ここでは翻訳）にチューニングされる際の具体的な処理の流れとその背後にある数式です。<br>
#### トークン化
T5は、トークナイザーと呼ばれるコンポーネントを使用して、入力されたテキストをトークン（単語や部分単語）に分割します。<br>
そして、これらのトークンをモデルが理解できる形式、つまりIDに変換します。<br>
これには、SentencePieceというアルゴリズムを使用します。<br>
SentencePieceは、トレーニングデータに基づいてトークンのIDを決定します。<br>
したがって、異なるトレーニングデータセットは、異なるトークン-IDマッピングを生成します。<br>
具体的な数値と数式を使用してこのプロセスを説明すると次のようになります：<br>

1. **トークンの頻度の計算**：
まず、SentencePieceはテキストコーパス全体を見て、各単語や部分単語の出現頻度を計算します。<br>
これは、各トークンがテキスト中に何回出現するかを数えることで行われます。<br>
数式で表すと、トークン $ t $ の頻度 $ f(t) $ は次のように計算されます：<br>

    $$
    f(t) = \text{Count of } t \text{ in the corpus}
    $$

    例えば、"the"というトークンがテキストコーパス中に1000回出現した場合、その頻度 $ f(\text{"the"}) $ は1000となります。<br>

2. **トークンのランク付け**：
次に、これらのトークンを出現頻度に基づいてランク付けします。<br>出現頻度が高いトークンは低いIDを、出現頻度が低いトークンは高いIDを割り当てられます。<br>
数式で表すと、トークン $ t $ のランク $ r(t) $ は次のように計算されます：<br>

    $$
    r(t) = \text{Rank of } t \text{ based on } f(t)
    $$

    例えば、"the"が最も頻繁に出現するトークンだった場合、そのランク $ r(\text{"the"}) $ は1となります。<br>

3. **IDの割り当て**：最後に、各トークンに対して、そのランクに基づいて一意のIDが割り当てられます。<br>
数式で表すと、トークン $ t $ のID $ id(t) $ は次のように決定されます：<br>

    $$
    id(t) = r(t)
    $$

    したがって、"the"のID $ id(\text{"the"}) $ は1となります。<br>

特殊なトークン（例えば、文の開始や終了を示すトークン、未知のトークンを示すトークンなど）に対しても一意のIDが割り当てられます。<br>
これらのトークンの扱いは、通常の単語や部分単語のトークンとは異なります。<br>

1. **未知のトークン**：
未知のトークンは、トレーニングデータに存在しない新しい単語や部分単語を指します。<br>
SentencePieceは、これらの未知のトークンを特殊な未知トークン `<unk>` にマッピングします。<br>
この `<unk>` トークンは、語彙の一部として事前に定義され、一意のIDが割り当てられます。<br>
例えば、語彙に "apple" という単語が存在しない場合、"apple" は未知のトークンとして扱われ、 `<unk>` のIDにマッピングされます。<br>

2. **特殊なトークン**：
特殊なトークンは、特定の目的のために定義されたトークンを指します。<br>
これには、文の開始を示す `<s>`、文の終了を示す `</s>`、パディングを示す `<pad>` などが含まれます。<br>
これらの特殊なトークンは、語彙の一部として事前に定義され、一意のIDが割り当てられます。<br>
例えば、文の開始を示す `<s>` トークンは、特定のID（例えば、0）にマッピングされます。<br>

これらの特殊なトークンと未知のトークンのIDは、通常のトークンのIDとは独立して定義されます。<br>
したがって、これらのトークンのIDは、出現頻度やランク付けのプロセスには影響されません。<br>

具体的な数値を使用して説明すると、以下のようなマッピングが考えられます：<br>

- `<s>`（文の開始）：0
- `</s>`（文の終了）：1
- `<unk>`（未知のトークン）：2
- `<pad>`（パディング）：3
- "the"：4
- "a"：5
- "and"：6
- ...

このように、特殊なトークンと未知のトークンは、通常のトークンよりも低いIDが割り当てられます。<br>
#### トークナイザの学習
SentencePieceのトレーニングプロセスは、バイトペアエンコーディング（BPE）アルゴリズムまたはunigram language modelアルゴリズムを使用して行われます。<br>
これらのアルゴリズムは、テキストコーパスからサブワードを抽出し、それらにIDを割り当てるためのルールを学習します。<br>

以下に、これらのアルゴリズムの詳細な説明を提供します：

1. **バイトペアエンコーディング（BPE）**：
BPEアルゴリズムは、最も頻繁に共起する文字のペアを見つけ、それらを新しいサブワードとして結合します。<br>
このプロセスを繰り返すことで、最終的には単語全体をカバーするサブワードのセットが作成されます。<br>

    BPEのトレーニングプロセスは以下の通りです：<br>

    - 初期段階では、全てのUnicode文字を個別のトークンとして扱います。<br>
    これにより、どのようなテキストもトークン化できるようになります。<br>
    - 次に、テキストコーパス全体をスキャンし、各トークンの出現頻度を計算します。<br>
    この出現頻度は、各トークンがテキスト中に何回出現するかを数えることで得られます。<br>
    - その後、最も頻繁に共起するトークンのペアを見つけ、それらを新しいトークンとして結合します。<br>
    この新しいトークンは、元の2つのトークンが連続して出現するすべての場所でそれらを置き換えます。<br>
    - このプロセスを繰り返し、最終的には語彙サイズが指定した数になるまで新しいトークンを作り続けます。<br>

2. **Unigram Language Model**：
Unigram language modelアルゴリズムは、サブワードの確率的な生成モデルを使用します。<br>
このモデルは、テキストコーパスをスキャンして各サブワードの出現頻度を計算し、それを基にサブワードの生成確率を推定します。<br>

    Unigram Language Modelのトレーニングプロセスは以下の通りです：<br>

    - 初期段階では、全てのUnicode文字を個別のトークンとして扱います。<br>これにより、どのようなテキストもトークン化できるようになります。<br>
    - 次に、テキストコーパス全体をスキャンし、各サブワードの出現頻度を計算します。<br>
    この出現頻度は、各サブワードがテキスト中に何回出現するかを数えることで得られます。<br>
    - その後、各サブワードが生成される確率を推定します。<br>この確率は、そのサブワードの出現頻度と、それを生成するために必要な他のサブワードの出現頻度に基づいています。<br>
    - このプロセスを繰り返し、最終的には語彙サイズが指定した数になるまで新しいサブワードを作り続けます。<br>

これらのアルゴリズムは、テキストコーパスに基づいてサブワードを動的に生成します。<br>
したがって、異なるテキストコーパスは異なるサブワードのセットを生成します。<br>

また、これらのアルゴリズムは、特定の言語やスクリプトに依存しないため、多言語のテキストや異なる文字セットを含むテキストに対しても使用することができます。<br>
#### エンコーダー
T5のエンコーダーは、トークン化されたテキストをベクトル表現に変換します。<br>
これは、自然言語処理モデルがテキストを理解し、それに基づいて予測を行うための基礎となるプロセスです。<br>

エンコーダーは、以下のステップでトークンをベクトル表現に変換します：<br>

1. **埋め込み層**：
トークン化された入力 $ x = [x_1, x_2, ..., x_n] $ は、埋め込み層を通過します。<br>
これは、トークン（サブワード）を固定長のベクトルに変換します。<br>
このベクトルは、トークンの意味を表現します。<br>
具体的には、各トークン $ x_i $ は、埋め込み行列 $ W_e $ を用いてベクトル $ E(x_i) = W_e x_i $ に変換されます。<br>
これは $ E(x) = [E(x_1), E(x_2), ..., E(x_n)] $ と表現できます。<br>
埋め込みは学習可能なパラメータで、モデルの学習中に更新されます。<br>

2. **位置エンコーディング**：
これは、各トークンの位置情報をモデルに提供します。<br>
位置エンコーディング\(P\)が埋め込みベクトルに追加されます。<br>
$ E(x_i) = E(x_i) + P(i) $
位置エンコーディングは学習可能なパラメータとして扱われます。<br>

3. **自己注意メカニズム**：
これは、各トークンが他のすべてのトークンとどのように関連しているかをモデルが学習するためのものです。<br>
自己注意メカニズムは、各トークンの新しいベクトル表現を生成します。<br>
具体的には、自己注意メカニズムは、各トークンのベクトル $ E(x_i) $ 、すべてのトークンのベクトル $ E(x_j) $ 、および対応する注意重み $ a_{ij} $に基づいて計算されます。<br>
注意重みは、トークン $ i $ がトークン $ j $ にどの程度注意を払うべきかを示します。<br>
これらの重みは、以下の式を使用して計算されます：<br>

$$
a_{ij} = \frac{\exp(e_{ij})}{\sum_{k=1}^{n}\exp(e_{ik})}
$$

ここで、 $ e_{ij} $ はエネルギー関数で、トークン $ i $ と $ j $ の間の関連性を示します。<br>
これは通常、以下のように計算されます：<br>

$$
e_{ij} = E(x_i)^T W_a E(x_j)
$$

ここで、$ W $ は学習可能な重み行列です。<br>
そして、新しいベクトル表現は以下のように計算されます：<br>

$$
z_i = \sum_{j=1}^{n} a_{ij} x_j
$$

4. **フィードフォワードネットワーク**：
各トークンのベクトル表現をさらに変換します。<br>
フィードフォワードネットワークは、以下の形式を持つことが一般的です：<br>

$$
FFN(x) = max(0, xW_1 + b_1)W_2 + b_2
$$

ここで、$ W_1 $、$ W_2 $、$ b_1 $、および $ b_2 $は学習可能なパラメータです。<br>
#### デコーダー
T5のデコーダーは、エンコーダーの出力と自分自身の過去の出力に基づいて次のトークンを予測します。<br>
これは、エンコーダー-デコーダー構造の一部で、自然言語生成タスクにおいて重要な役割を果たします。<br>

デコーダーは以下のステップで次のトークンを予測します：<br>

1. **埋め込み層**：
まず、デコーダーは自分自身の過去の出力（すでに生成されたトークン）を埋め込み層に通します。<br>
これは、トークンをベクトルに変換します。<br>
これはエンコーダーと同じです。<br>

2. **位置エンコーディング**：
次に、位置エンコーディングが埋め込みベクトルに追加されます。<br>
これは、各トークンの位置情報をモデルに提供します。<br>
これもエンコーダーと同じです。<br>

3. **マスク付き自己注意メカニズム**：
デコーダーは自己注意メカニズムを使用しますが、未来のトークンに対する情報をマスクします（これは、生成されていないトークンに対する情報をブロックします）。<br>
これにより、モデルは過去の出力だけに基づいて次のトークンを予測します。<br>
これもエンコーダーの自己注意メカニズムと同じですが、未来のトークンに対する情報をマスクします。<br>

4. **エンコーダー-デコーダー注意メカニズム**：
デコーダーは、エンコーダーの出力に対する注意メカニズムも使用します。<br>
これにより、デコーダーはエンコーダーの出力と自分自身の過去の出力の両方を考慮して次のトークンを予測します。<br>
具体的には、エンコーダーの出力 $ z_i $ とデコーダーの過去の出力 $ y_j $ に基づいて新しいベクトル表現を計算します：<br>

$$
a_{ij} = \frac{\exp(e_{ij})}{\sum_{k=1}^{n}\exp(e_{ik})}
$$

ここで、$ e_{ij} $ はエネルギー関数で、デコーダーのトークン $ j $ とエンコーダーのトークン $ i $の間の関連性を示します。<br>
これは通常、以下のように計算されます：<br>

$$
e_{ij} = y_j^T W z_i
$$

ここで、$ W $ は学習可能な重み行列です。<br>
そして、新しいベクトル表現は以下のように計算されます：<br>

$$
y'_j = \sum_{i=1}^{n} a_{ij} z_i
$$

5. **フィードフォワードネットワーク**：
最後に、各トークンはフィードフォワードネットワークを通過します。<br>
これは、各トークンのベクトル表現をさらに変換します。<br>
これもエンコーダーと同じです。<br>
6. **ソフトマックス関数**：
最後に、デコーダーは、これらのベクトル表現を使用して次のトークンを予測します。<br>
デコーダーの出力は、ソフトマックス関数を通じて確率分布に変換されます。<br>
これは、各可能な出力トークンの確率を表すベクトルを生成します。<br>
この確率分布は、次のトークンを生成するために使用されます。<br>
ソフトマックス関数は以下の形式を持ちます：<br>

$$
\text{softmax}(x)_i = \frac{\exp(x_i)}{\sum_{j=1}^{N}\exp(x_j)}
$$

ここで、$ x $ はデコーダーの出力ベクトル（フィードフォワードネットワークからの出力）、$ N $ は可能な出力トークンの数、そして $ i $ は特定のトークンを指します。<br>
この関数は、各トークンのスコアを確率に変換します。<br>
すべてのトークンの確率の合計は1になります。<br>
この確率分布は、次のトークンを選択するために使用されます。<br>
最も高い確率を持つトークンが選択されることが最も一般的ですが、ランダムな要素が含まれることもあります（これは「サンプリング」または「ビームサーチ」と呼ばれます）。<br>
#### T5重みの更新
T5モデルは以下のステップで重み更新を行います。<br>

1. **クロスエントロピー損失**：
予測確率分布と真のラベルの確率分布との間のクロスエントロピー損失を計算します。<br>
これは、モデルの予測が真のラベルからどれだけ離れているかを測定する一般的な方法です。<br>
クロスエントロピー損失は以下の式で定義されます：<br>

$$
H(p, q) = -\sum_{i} p(i) \log q(i)
$$

ここで、$ p $ は真のラベルの確率分布、$ q $ はモデルによって予測された確率分布、そして $ i $ は特定のラベルを指します。<br>
具体的には、$ p(i) $ は真のラベルが $ i $ である場合に1、それ以外の場合に0となるワンホットエンコーディングです。<br>
一方、$ q(i) $ はモデルが出力したソフトマックス関数の結果、つまりトークン $ i $ の予測確率です。<br>
この損失関数の値が小さいほど、モデルの予測が真のラベルに近いことを意味します。<br>
モデルの学習中には、この損失を最小化するようにモデルのパラメータが更新されます。<br>

1. **損失の勾配**：
T5は、クロスエントロピー損失の勾配を計算し、これを使用してモデルのパラメータ（エンコーダーとデコーダーの重み）を更新します。<br>
これは、勾配降下法という最適化アルゴリズムの一部です。<br>
勾配は、損失関数のパラメータに対する偏微分から計算されます。<br>
具体的には、各パラメータ $ w $ に対して、損失関数 $ L $ の勾配は以下のように計算されます：<br>

$$
\frac{\partial L}{\partial w}
$$

これは、パラメータ $ w $ を少し変化させたときに損失 $ L $ がどれだけ変化するかを示します。<br>
勾配が正の場合、パラメータを増やすと損失が増えることを意味します。<br>
逆に、勾配が負の場合、パラメータを増やすと損失が減少します。<br>
勾配降下法では、パラメータは以下のように更新されます：<br>

$$
w := w - \eta \frac{\partial L}{\partial w}
$$

ここで、$ \eta $ は学習率と呼ばれるハイパーパラメータで、パラメータの更新の大きさを制御します。<br>
学習率は通常、0.1や0.01などの小さな値に設定されます。<br>

この更新は、損失が最小となるパラメータを見つけるために、一連のエポック（すべての訓練データを一度通過すること）にわたって繰り返されます。<br>