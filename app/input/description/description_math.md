#### ニューラルネットワークの処理の流れ
下記は、ニューラルネットワークの順伝播と逆伝播のプロセス、そしてパラメータの更新の流れを表す数式の具体例です。

1. パラメータの初期化

重みとバイアスを次のように定義します：

$$
W^{(1)} = \begin{bmatrix} 0.1 & 0.2 \\ 0.3 & 0.4 \end{bmatrix}, \quad b^{(1)} = \begin{bmatrix} 0.01 \\ 0.02 \end{bmatrix}
$$

$$
W^{(2)} = \begin{bmatrix} 0.5 & 0.6 \end{bmatrix}, \quad b^{(2)} = 0.03
$$

2. 入力と真のラベルの定義

入力ベクトルと真のラベル（期待する値）を次のように定義します：

$$
x = \begin{bmatrix} 0.7 \\ 0.8 \end{bmatrix}, \quad y_{\text{true}} = 1
$$

3. 順伝播

隠れ層と出力層を次のように計算します：

$$
隠れ層: h = \tanh(W^{(1)}x + b^{(1)})
$$

$$
出力層: y_{\text{pred}} = \sigma(W^{(2)}h + b^{(2)})
$$

ここで、$ \sigma $ はシグモイド関数です。

4. 損失の計算

クロスエントロピー損失を次のように計算します：

$$
L = -y_{\text{true}} \log(y_{\text{pred}}) - (1 - y_{\text{true}}) \log(1 - y_{\text{pred}})
$$

5. 勾配の計算

損失関数の勾配を次のように計算します：

$$
\frac{\partial L}{\partial W^{(1)}} = (W^{(2)} (y_{\text{pred}} - y_{\text{true}})) \cdot (1 - h^2) \cdot x^T
$$

$$
\frac{\partial L}{\partial b^{(1)}} = (W^{(2)} (y_{\text{pred}} - y_{\text{true}})) \cdot (1 - h^2)
$$

$$
\frac{\partial L}{\partial W^{(2)}} = (y_{\text{pred}} - y_{\text{true}}) \cdot h
$$

$$
\frac{\partial L}{\partial b^{(2)}} = y_{\text{pred}} - y_{\text{true}}
$$

6. パラメータの更新
パラメータの更新は次のように行います：

$$
W^{(1)}_{\text{new}} = W^{(1)} - \eta \frac{\partial L}{\partial W^{(1)}}
$$

$$
b^{(1)}_{\text{new}} = b^{(1)} - \eta \frac{\partial L}{\partial b^{(1)}}
$$

$$
W^{(2)}_{\text{new}} = W^{(2)} - \eta \frac{\partial L}{\partial W^{(2)}}
$$

$$
b^{(2)}_{\text{new}} = b^{(2)} - \eta \frac{\partial L}{\partial b^{(2)}}
$$

ここで、$ \eta $ は学習率です。
#### T5での学習の流れ
以下の図は、T5モデルがタスクを理解し、重みを更新する流れを示しています。
![T5 Model Flow](https://showme.redstarplugin.com/d/q9aPYPlK)
1. 入力値をトークンに変換します。
これは一般的には単語やサブワードのトークン化によって行われます。
具体的な数式はありませんが、トークン化の結果、入力テキスト "Translate to Japanese: Thank you" はトークンのシーケンス ["Translate", "to", "Japanese", ":", "Thank", "you"] に変換されます。

2. トークンをエンコーダーに渡します。
エンコーダーはトークンをベクトル表現に変換します。
この変換は埋め込み行列 $ E $ を使用して行われ、各トークン $ t_i $ のベクトル表現 $ v_i $ は $ v_i = E[t_i] $ と計算されます。

3. エンコーダーは、トークンのベクトル表現に基づいて隠れ状態を計算します。これは自己注意メカニズムとフィードフォワードネットワークを使用して行われます。
自己注意メカニズムは以下の数式に基づいています：

   $$ Attention(Q, K, V) = softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V $$

   ここで、$ Q $、$ K $、$ V $ はクエリ、キー、値の行列であり、これらはエンコーダーの入力トークンから計算されます。また、$ d_k $ はキーの次元数です。

1. エンコーダーの出力（隠れ状態）はデコーダーに渡されます。

2. デコーダーはエンコーダーの出力と自分自身の過去の出力に基づいて次のトークンを予測します。
これはエンコーダーの自己注意メカニズムと同じ数式を使用しますが、クエリはデコーダーから、キーと値はエンコーダーから取得します。

6. デコーダーの出力はソフトマックス関数を通じて確率分布に変換されます。
これは各トークンの予測確率を表します。
ソフトマックス関数は以下の数式に基づいています：

   $$ \hat{y}_i = \frac{exp(z_i)}{\sum_j exp(z_j)} $$

   ここで、$ z_i $ はデコーダーの出力（ロジット）、$ \hat{y}_i $ は予測確率分布です。

7. 予測確率分布と真のラベルの確率分布との間のクロスエントロピー損失を計算します。
クロスエントロピー損失は以下の数式に基づいて計算されます：

   $$ L = -\sum_{i=1}^{n} y_i \log(\hat{y}_i) $$

   ここで、$ y_i $ は真のラベルの確率分布（one-hotエンコーディングされたラベル）、$ \hat{y}_i $ は予測確率分布、$ n $ はボキャブラリのサイズ（すなわち、可能なトークンの数）です。

8. クロスエントロピー損失の勾配を計算し、これを使用してモデルのパラメータ（エンコーダーとデコーダーの重み）を更新します。これはバックプロパゲーションアルゴリズムと勾配降下法（またはその派生形）を使用して行われます。具体的な数式は以下の通りです：

   $$ w_{new} = w_{old} - \alpha \frac{\partial L}{\partial w} $$

   ここで、$ w_{old} $ は更新前の重み、$ w_{new} $ は更新後の重み、$ \alpha $ は学習率（ステップサイズ）、$ \frac{\partial L}{\partial w} $ は損失関数の重みに対する勾配です。

以上が、T5モデルが特定のタスク（ここでは翻訳）にチューニングされる際の具体的な処理の流れとその背後にある数式です。
#### トークン化
T5は、トークナイザーと呼ばれるコンポーネントを使用して、入力されたテキストをトークン（単語や部分単語）に分割します。
そして、これらのトークンをモデルが理解できる形式、つまりIDに変換します。
これには、SentencePieceというアルゴリズムを使用します。
SentencePieceは、トレーニングデータに基づいてトークンのIDを決定します。
したがって、異なるトレーニングデータセットは、異なるトークン-IDマッピングを生成します。
具体的な数値と数式を使用してこのプロセスを説明すると次のようになります：

1. **トークンの頻度の計算**：
まず、SentencePieceはテキストコーパス全体を見て、各単語や部分単語の出現頻度を計算します。
これは、各トークンがテキスト中に何回出現するかを数えることで行われます。
数式で表すと、トークン $ t $ の頻度 $ f(t) $ は次のように計算されます：

    $$
    f(t) = \text{Count of } t \text{ in the corpus}
    $$

    例えば、"the"というトークンがテキストコーパス中に1000回出現した場合、その頻度 $ f(\text{"the"}) $ は1000となります。

2. **トークンのランク付け**：
次に、これらのトークンを出現頻度に基づいてランク付けします。出現頻度が高いトークンは低いIDを、出現頻度が低いトークンは高いIDを割り当てられます。
数式で表すと、トークン $ t $ のランク $ r(t) $ は次のように計算されます：

    $$
    r(t) = \text{Rank of } t \text{ based on } f(t)
    $$

    例えば、"the"が最も頻繁に出現するトークンだった場合、そのランク $ r(\text{"the"}) $ は1となります。

3. **IDの割り当て**：最後に、各トークンに対して、そのランクに基づいて一意のIDが割り当てられます。
数式で表すと、トークン $ t $ のID $ id(t) $ は次のように決定されます：

    $$
    id(t) = r(t)
    $$

    したがって、"the"のID $ id(\text{"the"}) $ は1となります。

特殊なトークン（例えば、文の開始や終了を示すトークン、未知のトークンを示すトークンなど）に対しても一意のIDが割り当てられます。
これらのトークンの扱いは、通常の単語や部分単語のトークンとは異なります。

1. **未知のトークン**：
未知のトークンは、トレーニングデータに存在しない新しい単語や部分単語を指します。
SentencePieceは、これらの未知のトークンを特殊な未知トークン `<unk>` にマッピングします。
この `<unk>` トークンは、語彙の一部として事前に定義され、一意のIDが割り当てられます。
例えば、語彙に "apple" という単語が存在しない場合、"apple" は未知のトークンとして扱われ、 `<unk>` のIDにマッピングされます。

2. **特殊なトークン**：
特殊なトークンは、特定の目的のために定義されたトークンを指します。
これには、文の開始を示す `<s>`、文の終了を示す `</s>`、パディングを示す `<pad>` などが含まれます。
これらの特殊なトークンは、語彙の一部として事前に定義され、一意のIDが割り当てられます。
例えば、文の開始を示す `<s>` トークンは、特定のID（例えば、0）にマッピングされます。

これらの特殊なトークンと未知のトークンのIDは、通常のトークンのIDとは独立して定義されます。
したがって、これらのトークンのIDは、出現頻度やランク付けのプロセスには影響されません。

具体的な数値を使用して説明すると、以下のようなマッピングが考えられます：

- `<s>`（文の開始）：0
- `</s>`（文の終了）：1
- `<unk>`（未知のトークン）：2
- `<pad>`（パディング）：3
- "the"：4
- "a"：5
- "and"：6
- ...

このように、特殊なトークンと未知のトークンは、通常のトークンよりも低いIDが割り当てられます。
#### トークナイザの学習
SentencePieceのトレーニングプロセスは、バイトペアエンコーディング（BPE）アルゴリズムまたはunigram language modelアルゴリズムを使用して行われます。
これらのアルゴリズムは、テキストコーパスからサブワードを抽出し、それらにIDを割り当てるためのルールを学習します。

以下に、これらのアルゴリズムの詳細な説明を提供します：

1. **バイトペアエンコーディング（BPE）**：
BPEアルゴリズムは、最も頻繁に共起する文字のペアを見つけ、それらを新しいサブワードとして結合します。
このプロセスを繰り返すことで、最終的には単語全体をカバーするサブワードのセットが作成されます。

    BPEのトレーニングプロセスは以下の通りです：

    - 初期段階では、全てのUnicode文字を個別のトークンとして扱います。
    これにより、どのようなテキストもトークン化できるようになります。
    - 次に、テキストコーパス全体をスキャンし、各トークンの出現頻度を計算します。
    この出現頻度は、各トークンがテキスト中に何回出現するかを数えることで得られます。
    - その後、最も頻繁に共起するトークンのペアを見つけ、それらを新しいトークンとして結合します。
    この新しいトークンは、元の2つのトークンが連続して出現するすべての場所でそれらを置き換えます。
    - このプロセスを繰り返し、最終的には語彙サイズが指定した数になるまで新しいトークンを作り続けます。

2. **Unigram Language Model**：
Unigram language modelアルゴリズムは、サブワードの確率的な生成モデルを使用します。
このモデルは、テキストコーパスをスキャンして各サブワードの出現頻度を計算し、それを基にサブワードの生成確率を推定します。

    Unigram Language Modelのトレーニングプロセスは以下の通りです：

    - 初期段階では、全てのUnicode文字を個別のトークンとして扱います。これにより、どのようなテキストもトークン化できるようになります。
    - 次に、テキストコーパス全体をスキャンし、各サブワードの出現頻度を計算します。
    この出現頻度は、各サブワードがテキスト中に何回出現するかを数えることで得られます。
    - その後、各サブワードが生成される確率を推定します。この確率は、そのサブワードの出現頻度と、それを生成するために必要な他のサブワードの出現頻度に基づいています。
    - このプロセスを繰り返し、最終的には語彙サイズが指定した数になるまで新しいサブワードを作り続けます。

これらのアルゴリズムは、テキストコーパスに基づいてサブワードを動的に生成します。
したがって、異なるテキストコーパスは異なるサブワードのセットを生成します。

また、これらのアルゴリズムは、特定の言語やスクリプトに依存しないため、多言語のテキストや異なる文字セットを含むテキストに対しても使用することができます。
#### エンコーダー・デコーダー
T5モデルのエンコーダとデコーダの動作を数式で表現すると以下のようになります。
ただし、具体的な数値はモデルの内部状態に依存するため、ここでは一般的な形式で表現します。

1. **エンコーダ**:
   - 入力シーケンス（テキスト）はトークン化されます。
   これは、テキストをモデルが理解できる形式に変換するプロセスです。
   例えば、入力文「Translate to Japanese: Thank you」はトークンのリスト ['Translate', 'to', 'Japanese', ':', 'Thank', 'you'] に変換されます。
   - トークン化された入力 $ x = [x_1, x_2, ..., x_n] $ は、埋め込みレイヤーを通過します。
   これは、各トークンを表現する高次元のベクトルに変換する操作です。
   具体的には、各トークン $ x_i $ は、埋め込み行列 $ W_e $ を用いてベクトル $ E(x_i) = W_e x_i $ に変換されます。
   これは $ E(x) = [E(x_1), E(x_2), ..., E(x_n)] $ と表現できます。
   - 埋め込みベクトル $ E(x) = [E(x_1), E(x_2), ..., E(x_n)] $ は、自己注意メカニズムを通過します。
   これは、各トークンが他のすべてのトークンとどのように関連しているかを捉えることができます。
   具体的には、各トークン $ E(x_i) $ の自己注意ベクトル $ A(E(x_i)) $ は、他のすべてのトークン $ E(x_j) $ の重み付き和として計算されます。
   重みは、$ E(x_i) $ と $ E(x_j) $ の内積に基づいて計算されます。
   これは $ A(E(x)) = [A(E(x_1)), A(E(x_2)), ..., A(E(x_n))] $ と表現できます。

2. **デコーダ**:
   - エンコーダの出力 $ A(E(x)) = [A(E(x_1)), A(E(x_2)), ..., A(E(x_n))] $ はデコーダに渡されます。
   デコーダもまた、トークン化、埋め込み、自己注意のステップを経て、エンコーダの出力に対する注意を計算します。
   具体的には、各トークン $ A(E(x_i)) $ のデコーダ出力 $ D(A(E(x_i))) $ は、他のすべてのトークン $ A(E(x_j)) $ の重み付き和として計算されます。
   重みは、$ A(E(x_i)) $ と $ A(E(x_j)) $ の内積に基づいて計算されます。
   これは $ D(A(E(x))) = [D(A(E(x_1))), D(A(E(x_2))), ..., D(A(E(x_n)))] $ と表現できます。

これらの操作は、全てのトークンに対して独立に行われ、全てのトークンが互いに影響を与えることで、文全体の意味を捉えることができます。
このプロセス全体を通じて、T5モデルは入力テキストを理解し、適切な出力（翻訳、要約、質問応答など）を生成します。
#### クロス注意メカニズム
デコーダが次のトークンを予測する際に使用する注意メカニズムは、クロス注意（Cross-Attention）と呼ばれます。
クロス注意メカニズムは、エンコーダの隠れ状態とデコーダの現在の状態を考慮して、次のトークンの予測にどのエンコーダのトークンを重視するかを決定します。

クロス注意メカニズムの数学的な詳細は以下の通りです：

1. **クエリ（Query）**：デコーダの現在の隠れ状態。
2. **キー（Key）**：エンコーダの各トークンの隠れ状態。
3. **バリュー（Value）**：エンコーダの各トークンの隠れ状態。

クロス注意スコアは、クエリとキーの内積として計算され、次にソフトマックス関数を適用して確率分布（注意重み）を得ます。これらの注意重みは、バリュー（エンコーダの隠れ状態）と組み合わせられ、加重平均として計算されます。この加重平均は、デコーダの次の隠れ状態を生成するために使用されます。

数式で表すと以下のようになります：

1. **注意スコアの計算**：$ score = Query \cdot Key^T $
2. **注意重みの計算**：$ weights = softmax(score) $
3. **コンテキストベクトルの計算**：$ context = weights \cdot Value $

ここで、$ Query $、$ Key $、$ Value $ はそれぞれデコーダの現在の隠れ状態、エンコーダの各トークンの隠れ状態を表します。
また、$ softmax $関数は、スコアを正規化して確率分布に変換します。

このコンテキストベクトルは、エンコーダの各トークンの情報を考慮したデコーダの新しい隠れ状態を表します。
この新しい隠れ状態は、次のトークンを予測するために使用されます。

このプロセスは、エンコーダの各トークンがデコーダの予測にどのように影響を与えるかをモデルに理解させるためのものです。特に、タスク指定トークン（この場合は「translate English to Japanese」）は、モデルがどのようなタスクを実行するべきかを理解するための重要な情報を提供します。
#### T5重みの更新
T5モデルの重み更新の流れは以下のようになります。

T5モデルはテキストのシーケンスを入力として受け取り、各時間ステップ（または位置）での次のトークンを予測します。
訓練中、モデルは実際の次のトークンと予測されたトークンの確率分布の間のクロスエントロピー損失を計算します。

クロスエントロピー損失は次のように定義されます：

$$
L = -\frac{1}{N}\sum_{i=1}^{N} y_i \log(p_i)
$$

ここで、
- $ N $ はバッチサイズ（またはシーケンス長）です。
- $ y_i $ は真のラベル（one-hotエンコーディング形式）です。
- $ p_i $ はモデルによって予測された確率です。

この損失は、モデルが正しいトークンを予測する確率を最大化するように設計されています。
つまり、モデルが真のトークンを高い確率で予測すると損失は小さくなり、逆に真のトークンを低い確率で予測すると損失は大きくなります。

次に、この損失を用いてモデルのパラメータ（重み）を更新します。
これはバックプロパゲーションと呼ばれるアルゴリズムを使用して行われます。
具体的には、損失関数の勾配（つまり、パラメータに対する損失の微分）を計算し、その勾配の方向にパラメータを少しずつ移動させます。
これにより、損失が最小化されるようにパラメータが調整されます。

パラメータの更新は以下のような式で行われます：

$$
w = w - \eta \frac{\partial L}{\partial w}
$$

ここで、
- $ w $ は更新されるパラメータ（重み）です。
- $ \eta $ は学習率と呼ばれるハイパーパラメータで、パラメータの更新の大きさを制御します。
- $ \frac{\partial L}{\partial w} $ は損失関数の勾配（つまり、パラメータに対する損失の微分）です。

このプロセスは、全体的な損失が十分に小さくなる、または他の停止条件が満たされるまで繰り返されます。