#### ニューラルネットワークの処理の流れ
下記は、ニューラルネットワークの順伝播と逆伝播のプロセス、そしてパラメータの更新の流れを表す数式の具体例です。

1. パラメータの初期化

重みとバイアスを次のように定義します：

$$
W^{(1)} = \begin{bmatrix} 0.1 & 0.2 \\ 0.3 & 0.4 \end{bmatrix}, \quad b^{(1)} = \begin{bmatrix} 0.01 \\ 0.02 \end{bmatrix}
$$

$$
W^{(2)} = \begin{bmatrix} 0.5 & 0.6 \end{bmatrix}, \quad b^{(2)} = 0.03
$$

2. 入力と真のラベルの定義

入力ベクトルと真のラベル（期待する値）を次のように定義します：

$$
x = \begin{bmatrix} 0.7 \\ 0.8 \end{bmatrix}, \quad y_{\text{true}} = 1
$$

3. 順伝播

隠れ層と出力層を次のように計算します：

$$
隠れ層: h = \tanh(W^{(1)}x + b^{(1)})
$$

$$
出力層: y_{\text{pred}} = \sigma(W^{(2)}h + b^{(2)})
$$

ここで、$ \sigma $ はシグモイド関数です。

4. 損失の計算

クロスエントロピー損失を次のように計算します：

$$
L = -y_{\text{true}} \log(y_{\text{pred}}) - (1 - y_{\text{true}}) \log(1 - y_{\text{pred}})
$$

5. 勾配の計算

損失関数の勾配を次のように計算します：

$$
\frac{\partial L}{\partial W^{(1)}} = (W^{(2)} (y_{\text{pred}} - y_{\text{true}})) \cdot (1 - h^2) \cdot x^T
$$

$$
\frac{\partial L}{\partial b^{(1)}} = (W^{(2)} (y_{\text{pred}} - y_{\text{true}})) \cdot (1 - h^2)
$$

$$
\frac{\partial L}{\partial W^{(2)}} = (y_{\text{pred}} - y_{\text{true}}) \cdot h
$$

$$
\frac{\partial L}{\partial b^{(2)}} = y_{\text{pred}} - y_{\text{true}}
$$

6. パラメータの更新
パラメータの更新は次のように行います：

$$
W^{(1)}_{\text{new}} = W^{(1)} - \eta \frac{\partial L}{\partial W^{(1)}}
$$

$$
b^{(1)}_{\text{new}} = b^{(1)} - \eta \frac{\partial L}{\partial b^{(1)}}
$$

$$
W^{(2)}_{\text{new}} = W^{(2)} - \eta \frac{\partial L}{\partial W^{(2)}}
$$

$$
b^{(2)}_{\text{new}} = b^{(2)} - \eta \frac{\partial L}{\partial b^{(2)}}
$$

ここで、$ \eta $ は学習率です。
#### T5での学習の流れ
以下の図は、T5モデルがタスクを理解し、重みを更新する流れを示しています。
![T5 Model Flow](https://showme.redstarplugin.com/d/q9aPYPlK)
1. 入力値をトークンに変換します。
これは一般的には単語やサブワードのトークン化によって行われます。
具体的な数式はありませんが、トークン化の結果、入力テキスト "Translate to Japanese: Thank you" はトークンのシーケンス ["Translate", "to", "Japanese", ":", "Thank", "you"] に変換されます。

2. トークンをエンコーダーに渡します。
エンコーダーはトークンをベクトル表現に変換します。
この変換は埋め込み行列 $ E $ を使用して行われ、各トークン $ t_i $ のベクトル表現 $ v_i $ は $ v_i = E[t_i] $ と計算されます。

3. エンコーダーは、トークンのベクトル表現に基づいて隠れ状態を計算します。これは自己注意メカニズムとフィードフォワードネットワークを使用して行われます。
自己注意メカニズムは以下の数式に基づいています：

   $$ Attention(Q, K, V) = softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V $$

   ここで、$ Q $、$ K $、$ V $ はクエリ、キー、値の行列であり、これらはエンコーダーの入力トークンから計算されます。また、$ d_k $ はキーの次元数です。

1. エンコーダーの出力（隠れ状態）はデコーダーに渡されます。

2. デコーダーはエンコーダーの出力と自分自身の過去の出力に基づいて次のトークンを予測します。
これはエンコーダーの自己注意メカニズムと同じ数式を使用しますが、クエリはデコーダーから、キーと値はエンコーダーから取得します。

6. デコーダーの出力はソフトマックス関数を通じて確率分布に変換されます。
これは各トークンの予測確率を表します。
ソフトマックス関数は以下の数式に基づいています：

   $$ \hat{y}_i = \frac{exp(z_i)}{\sum_j exp(z_j)} $$

   ここで、$ z_i $ はデコーダーの出力（ロジット）、$ \hat{y}_i $ は予測確率分布です。

7. 予測確率分布と真のラベルの確率分布との間のクロスエントロピー損失を計算します。
クロスエントロピー損失は以下の数式に基づいて計算されます：

   $$ L = -\sum_{i=1}^{n} y_i \log(\hat{y}_i) $$

   ここで、$ y_i $ は真のラベルの確率分布（one-hotエンコーディングされたラベル）、$ \hat{y}_i $ は予測確率分布、$ n $ はボキャブラリのサイズ（すなわち、可能なトークンの数）です。

8. クロスエントロピー損失の勾配を計算し、これを使用してモデルのパラメータ（エンコーダーとデコーダーの重み）を更新します。これはバックプロパゲーションアルゴリズムと勾配降下法（またはその派生形）を使用して行われます。具体的な数式は以下の通りです：

   $$ w_{new} = w_{old} - \alpha \frac{\partial L}{\partial w} $$

   ここで、$ w_{old} $ は更新前の重み、$ w_{new} $ は更新後の重み、$ \alpha $ は学習率（ステップサイズ）、$ \frac{\partial L}{\partial w} $ は損失関数の重みに対する勾配です。

以上が、T5モデルが特定のタスク（ここでは翻訳）にチューニングされる際の具体的な処理の流れとその背後にある数式です。
#### エンコーダー・デコーダー
T5モデルのエンコーダとデコーダの動作を数式で表現すると以下のようになります。
ただし、具体的な数値はモデルの内部状態に依存するため、ここでは一般的な形式で表現します。

1. **エンコーダ**:
   - 入力シーケンス（テキスト）はトークン化されます。
   これは、テキストをモデルが理解できる形式に変換するプロセスです。
   例えば、入力文「Translate to Japanese: Thank you」はトークンのリスト ['Translate', 'to', 'Japanese', ':', 'Thank', 'you'] に変換されます。
   - トークン化された入力 $ x = [x_1, x_2, ..., x_n] $ は、埋め込みレイヤーを通過します。
   これは、各トークンを表現する高次元のベクトルに変換する操作です。
   具体的には、各トークン $ x_i $ は、埋め込み行列 $ W_e $ を用いてベクトル $ E(x_i) = W_e x_i $ に変換されます。
   これは $ E(x) = [E(x_1), E(x_2), ..., E(x_n)] $ と表現できます。
   - 埋め込みベクトル $ E(x) = [E(x_1), E(x_2), ..., E(x_n)] $ は、自己注意メカニズムを通過します。
   これは、各トークンが他のすべてのトークンとどのように関連しているかを捉えることができます。
   具体的には、各トークン $ E(x_i) $ の自己注意ベクトル $ A(E(x_i)) $ は、他のすべてのトークン $ E(x_j) $ の重み付き和として計算されます。
   重みは、$ E(x_i) $ と $ E(x_j) $ の内積に基づいて計算されます。
   これは $ A(E(x)) = [A(E(x_1)), A(E(x_2)), ..., A(E(x_n))] $ と表現できます。

2. **デコーダ**:
   - エンコーダの出力 $ A(E(x)) = [A(E(x_1)), A(E(x_2)), ..., A(E(x_n))] $ はデコーダに渡されます。
   デコーダもまた、トークン化、埋め込み、自己注意のステップを経て、エンコーダの出力に対する注意を計算します。
   具体的には、各トークン $ A(E(x_i)) $ のデコーダ出力 $ D(A(E(x_i))) $ は、他のすべてのトークン $ A(E(x_j)) $ の重み付き和として計算されます。
   重みは、$ A(E(x_i)) $ と $ A(E(x_j)) $ の内積に基づいて計算されます。
   これは $ D(A(E(x))) = [D(A(E(x_1))), D(A(E(x_2))), ..., D(A(E(x_n)))] $ と表現できます。

これらの操作は、全てのトークンに対して独立に行われ、全てのトークンが互いに影響を与えることで、文全体の意味を捉えることができます。
このプロセス全体を通じて、T5モデルは入力テキストを理解し、適切な出力（翻訳、要約、質問応答など）を生成します。
#### クロス注意メカニズム
デコーダが次のトークンを予測する際に使用する注意メカニズムは、クロス注意（Cross-Attention）と呼ばれます。
クロス注意メカニズムは、エンコーダの隠れ状態とデコーダの現在の状態を考慮して、次のトークンの予測にどのエンコーダのトークンを重視するかを決定します。

クロス注意メカニズムの数学的な詳細は以下の通りです：

1. **クエリ（Query）**：デコーダの現在の隠れ状態。
2. **キー（Key）**：エンコーダの各トークンの隠れ状態。
3. **バリュー（Value）**：エンコーダの各トークンの隠れ状態。

クロス注意スコアは、クエリとキーの内積として計算され、次にソフトマックス関数を適用して確率分布（注意重み）を得ます。これらの注意重みは、バリュー（エンコーダの隠れ状態）と組み合わせられ、加重平均として計算されます。この加重平均は、デコーダの次の隠れ状態を生成するために使用されます。

数式で表すと以下のようになります：

1. **注意スコアの計算**：$ score = Query \cdot Key^T $
2. **注意重みの計算**：$ weights = softmax(score) $
3. **コンテキストベクトルの計算**：$ context = weights \cdot Value $

ここで、$ Query $、$ Key $、$ Value $ はそれぞれデコーダの現在の隠れ状態、エンコーダの各トークンの隠れ状態を表します。
また、$ softmax $関数は、スコアを正規化して確率分布に変換します。

このコンテキストベクトルは、エンコーダの各トークンの情報を考慮したデコーダの新しい隠れ状態を表します。
この新しい隠れ状態は、次のトークンを予測するために使用されます。

このプロセスは、エンコーダの各トークンがデコーダの予測にどのように影響を与えるかをモデルに理解させるためのものです。特に、タスク指定トークン（この場合は「translate English to Japanese」）は、モデルがどのようなタスクを実行するべきかを理解するための重要な情報を提供します。
#### T5重みの更新
T5モデルの重み更新の流れは以下のようになります。

T5モデルはテキストのシーケンスを入力として受け取り、各時間ステップ（または位置）での次のトークンを予測します。
訓練中、モデルは実際の次のトークンと予測されたトークンの確率分布の間のクロスエントロピー損失を計算します。

クロスエントロピー損失は次のように定義されます：

$$
L = -\frac{1}{N}\sum_{i=1}^{N} y_i \log(p_i)
$$

ここで、
- $ N $ はバッチサイズ（またはシーケンス長）です。
- $ y_i $ は真のラベル（one-hotエンコーディング形式）です。
- $ p_i $ はモデルによって予測された確率です。

この損失は、モデルが正しいトークンを予測する確率を最大化するように設計されています。
つまり、モデルが真のトークンを高い確率で予測すると損失は小さくなり、逆に真のトークンを低い確率で予測すると損失は大きくなります。

次に、この損失を用いてモデルのパラメータ（重み）を更新します。
これはバックプロパゲーションと呼ばれるアルゴリズムを使用して行われます。
具体的には、損失関数の勾配（つまり、パラメータに対する損失の微分）を計算し、その勾配の方向にパラメータを少しずつ移動させます。
これにより、損失が最小化されるようにパラメータが調整されます。

パラメータの更新は以下のような式で行われます：

$$
w = w - \eta \frac{\partial L}{\partial w}
$$

ここで、
- $ w $ は更新されるパラメータ（重み）です。
- $ \eta $ は学習率と呼ばれるハイパーパラメータで、パラメータの更新の大きさを制御します。
- $ \frac{\partial L}{\partial w} $ は損失関数の勾配（つまり、パラメータに対する損失の微分）です。

このプロセスは、全体的な損失が十分に小さくなる、または他の停止条件が満たされるまで繰り返されます。