## 専門用語説明
### 共通
#### AI（Artificial Intelligence）
人工知能。
人間の知能をコンピュータ上で再現したもの。
これには、学習、推論、問題解決、知識表現、認識などの能力が含まれます。
#### Machine Learning (ML)
機械学習。
AIの一分野で、コンピュータがデータから学習し、その学習結果に基づいて予測や判断を行う技術です。
#### Deep Learning (DL)
深層学習。
大量のデータから学習する際に、多層のニューラルネットワーク（人間の脳の神経細胞がつながっている構造を模倣したモデル）を用いることで高度な予測や判断を行う技術です。
#### Neural Network (NN)
ニューラルネットワーク。
人間の脳の神経細胞のつながり方を模倣した計算モデルのことです。
#### Supervised Learning
教師あり学習。
学習データ（入力）とそれに対する正解（出力）を用いて、入力から出力を予測するモデルを学習する方法です。
#### Unsupervised Learning
教師なし学習。
学習データのみを用いて、データの構造やパターンを発見するモデルを学習する方法です。
#### Reinforcement Learning (RL)
強化学習。
エージェントが環境と相互作用しながら報酬を最大化するような行動を学習する方法です。
#### Natural Language Processing (NLP)
自然言語処理。人間が日常的に使う言語（自然言語）をコンピュータに理解させる技術のことです。
#### Computer Vision
コンピュータビジョン。画像や動画から有用な情報を抽出する技術のことを指します。
#### Generative Adversarial Networks (GANs)
敵対的生成ネットワーク。二つのニューラルネットワークが競争することで新たなデータを生成する技術です。
#### n-gram
テキストや音声などの連続するデータの列に対する統計的モデルの一種です。
n-gramでは、n個の項目の連続（つまり、"gram"）を一つの単位として扱います。
このnは任意の正の整数で、例えば2ならビグラム（bigram）、3ならトリグラム（trigram）と呼ばれます。

以下にn-gramの例を示します。"I love dogs"という文があるとき：

- ユニグラム（1-gram）は ["I", "love", "dogs"]
- ビグラム（2-gram）は ["I love", "love dogs"]
- トリグラム（3-gram）は ["I love dogs"]
  
n-gramは特に自然言語処理（NLP）の分野で広く使われています。
たとえば、機械翻訳や音声認識などのタスクで、単語やフレーズがどれくらいの頻度で一緒に出現するかを調べるために用いられます。
また、テキストの類似性を測るためにも利用されます。
n-gramは、単語の出現確率を計算するためのシンプルなモデルであり、文脈を考慮した情報を捉える能力があります。

ただし、n-gramにはいくつかの限界もあります。
特に、nが大きくなるとデータのスパース性（データ中に存在する可能性のあるn-gramのうち、実際に観測されるn-gramが非常に少ないという問題）が問題となることがあります。
また、n-gramは直前のn-1個の単語の情報しか考慮しないため、長い範囲の依存関係を捉えることができません。
### ニューラルネットワーク
#### フィードフォワードニューラルネットワーク（Feedforward Neural Networks, FNN）
最も基本的な形式のニューラルネットワークで、情報が入力層から隠れ層を経由して出力層へ一方向に流れます。
フィードフォワードニューラルネットワークでは、各層は前の層からの出力を入力として受け取ります。
#### 畳み込みニューラルネットワーク（Convolutional Neural Networks, CNN）
画像認識や画像処理タスクに広く使用されるニューラルネットワーク。
CNNは畳み込み層を持つことで特徴を抽出し、最終的に全結合層でクラス分類などを行います。
#### リカレントニューラルネットワーク（Recurrent Neural Networks, RNN）
時系列データや自然言語など、順序性が重要なデータを扱うためのニューラルネットワーク。
過去の情報を保持し、それを次のステップの予測に利用します。
#### 長短期記憶（Long Short-Term Memory, LSTM）
RNNの一種で、長期的な依存関係を学習する能力が向上しています。
LSTMは「ゲート」と呼ばれる構造を用いて、情報の流れを調節します。
#### 変分オートエンコーダ（Variational Autoencoders, VAE）
教師なし学習を行うニューラルネットワークで、入力データを特定の形の潜在空間にマッピングします。VAEは、生成モデルとしても使用されます。
#### 生成敵対ネットワーク（Generative Adversarial Networks, GAN）
GANは二つのネットワーク、生成ネットワークと識別ネットワークから成ります。
成ネットワークは、データの分布を学習し、新たなデータを生成します。
一方、識別ネットワークは、入力されたデータが本物（実際のデータ）か偽物（生成ネットワークが生成したデータ）かを識別します。
これら二つのネットワークが互いに競争しながら学習を進めます。
#### トランスフォーマーネットワーク（Transformer Networks）
トランスフォーマーネットワークは自然言語処理（NLP）タスクに広く使われています。
特に、Attentionメカニズムを用いて、入力の異なる部分に対するモデルの焦点を動的に変えることができます。
### ハイパーパラメータ
#### 学習率（Learning Rate）
学習率は、ニューラルネットワークが学習する速度を制御する重要なハイパーパラメータです。
学習率が高すぎると、モデルは最適な解を見つけることが難しくなり、学習率が低すぎると、学習に時間がかかりすぎて効率的でない可能性があります。
適切な学習率を見つけるには実験が必要であり、一般的には学習の進行とともに学習率を下げる方法（学習率のスケジューリング）がよく用いられます。
#### バッチサイズ（Batch Size）
バッチサイズは、一度にネットワークに供給されるサンプルの数を決定します。
バッチサイズが大きいほど、一度に処理するデータが多くなり、学習の速度が向上しますが、メモリ使用量が増えます。
また、バッチサイズが大きすぎると、モデルの学習が不安定になる可能性があります。
一方、バッチサイズが小さい場合、学習はより安定しますが、学習に必要な時間が長くなります。
#### エポック数（Number of Epochs）
エポック数は、学習データ全体がネットワークを通過する回数を指します。
エポック数が多いほど、ネットワークは学習データをより多く見ることができますが、過学習のリスクが高まります。
エポック数が少なすぎると、モデルは学習データから十分な情報を学習できない可能性があります。一般的には、エポック数はモデルの性能が検証データセットで改善しなくなるまで増やします。
検証データのパフォーマンスが改善しなくなったときに学習を止めるというテクニックを早期停止といい、過学習を防ぐのに役立ちます。
#### 正則化パラメータ（Regularization Parameter）
正則化は過学習を防ぐための手法で、正則化パラメータはその正則化の強さを調節します。
正則化パラメータが大きいほど、モデルの重みを小さく保つことでモデルの複雑さを抑える効果が強まります。
しかし、パラメータが大きすぎるとモデルが学習データに適合しきれず、学習不足（underfitting）を起こす可能性があります。そのため、正則化パラメータの適切な値を見つけることが重要です。
#### 隠れ層の数（Number of Hidden Layers）および 隠れ層のユニット数（Number of Units in Hidden Layers）
ニューラルネットワークの隠れ層の数とその各層のユニット数（ニューロン数）は、モデルの複雑さと学習能力を決定します。
隠れ層やユニット数が多いほど、モデルは複雑な関数を学習する能力が高まります。
しかし、適切な数を超えて隠れ層やユニット数を増やすと、モデルは過学習を起こしやすくなり、学習データに対する精度は高くなりますが、新たなデータに対する精度は低下します。
#### 最適化アルゴリズム（Optimizer）
最適化アルゴリズムは、モデルのパラメータを更新し、学習データに対する誤差を最小化するための手法です。
一般的な最適化アルゴリズムには、SGD（確率的勾配降下法）、Adam、RMSpropなどがあります。
各アルゴリズムは特定の問題に対して最適な場合もあるため、どのアルゴリズムを使用するかは問題によります。
また、最適化アルゴリズムは学習率と密接に関連しています。
学習率は大きすぎると学習が不安定になり、小さすぎると学習が遅くなる可能性があります。
#### ドロップアウト率（Dropout Rate）
ドロップアウトは、過学習を防ぐための手法の一つで、ランダムにノードを「ドロップアウト」（無効化）することで、モデルの汎化能力を向上させることができます。
ドロップアウト率は、無効化するノードの割合を指定します。
ドロップアウト率が高いと学習が遅くなる可能性がありますが、低すぎると効果が薄れる可能性があります。
#### 活性化関数（Activation Function
活性化関数は、ニューロンの出力を決定する関数で、非線形性をモデルに導入する役割があります。
一般的な活性化関数には、ReLU（Rectified Linear Unit）、sigmoid、tanh、Leaky ReLUなどがあります。
適切な活性化関数の選択は、タスクとネットワークの構造によります。
ReLUは一般的に最初の選択肢とされていますが、一部のニューロンが「死ぬ」（出力がほぼゼロになる）ことがあります。
#### 重み初期化方法（Weight Initialization Method）
深層学習モデルのパラメータ（重みとバイアス）の初期値を設定する方法です。
ネットワークの構造（特に活性化関数）と相性の良い初期化を選ぶことが重要です。例えば、ReLU活性化関数を使用している場合、He初期化が適しています。また、タスクの性質とデータの分布も考慮に入れるべきです。
### ハイパーパラメータ詳細
#### 最適化アルゴリズム
下記のアルゴリズムはすべて異なる特性と利点を持ち、その適用は特定のタスクとデータセットに依存します。
また、最適化アルゴリズムの選択は、他のハイパーパラメータ（例えば、学習率、ミニバッチサイズ、エポック数など）と密接に関連しています。
それぞれのアルゴリズムには特定の使用時の注意があります。
たとえば、AdamやAdafactorなどのより高度なアルゴリズムは、パラメータの初期化や学習率の設定に特に注意を要します。
また、モデルのサイズと学習データの量によって、最適なアルゴリズムが変わることもあります。
このため、最適なアルゴリズムを選択するためには、異なるアルゴリズムとハイパーパラメータ設定を試すことが重要です
##### SGD (Stochastic Gradient Descent)
SGDは最も基本的な最適化アルゴリズムで、各ステップで学習データの小さなサブセット（ミニバッチ）を使用して勾配を計算します。
SGDの主な問題は、全てのパラメータに対して同じ学習率を使用するため、一部のパラメータが他のパラメータよりも速く収束する可能性があることです。
また、SGDはしばしば局所最小値に捕らわれやすいです。
##### Momentum
MomentumはSGDの改良版で、過去の勾配を累積して、パラメータ更新に"運動量"を与えます。
これにより、学習は全体的によりスムーズになり、局所最小値を超えて全体の最小値に到達する可能性が高まります。
##### Adagrad
Adagradは、各パラメータに対して異なる学習率を設定するアルゴリズムです。
これにより、一部のパラメータが他のパラメータよりも速く収束する問題が解決します。しかし、Adagradの問題は、学習率が徐々に減少し、最終的には0に近づくため、学習が停止する可能性があることです。
##### RMSProp
RMSPropはAdagradの問題を解決するためのアルゴリズムです。
RMSPropは、過去の全ての勾配ではなく、最近の一部の勾配のみを考慮します。
これにより、学習率が0に近づく問題を解消します。
##### Adam (Adaptive Moment Estimation)
AdamはMomentumとRMSPropのアイデアを組み合わせたアルゴリズムで、過去の勾配の一部を考慮しながら各パラメータに対して適応的な学習率を設定します。
Adamは多くのNLPタスクで最高の結果を提供し、そのためT5モデルの学習において一般的に推奨されます。
##### Adafactor
Adafactorは、トランスフォーマーモデルの学習のために設計された最適化アルゴリズムです。
Adafactorは、パラメータの2次モーメント（つまり、パラメータの平方の期待値）を近似的に保存し、それを使用して学習率を適応的に調整します。
Adafactorは、メモリ効率が非常に高いことが特徴であり、大規模なトランスフォーマーモデルの学習に特に適しています。
#### 活性化関数
活性化関数の選択は、モデルの性能に大きな影響を及ぼす可能性があります。
各活性化関数は特定の特性と利点を持ち、その適用は特定のタスクとモデルアーキテクチャに依存します。
また、活性化関数の選択は、他のハイパーパラメータ（例えば、学習率、ミニバッチサイズ、エポック数など）と密接に関連しています。
それぞれの活性化関数には特定の使用時の注意があります。
たとえば、ReLUやLeaky ReLUは、一部のニューロンが「死ぬ」可能性があるため、その影響を最小限に抑えるための対策が必要です。
また、SigmoidやTanhは勾配消失問題を引き起こす可能性があります。
GELUは一般的に良好な結果をもたらしますが、計算コストが高い可能性があります。
このため、最適な活性化関数を選択するためには、異なる活性化関数とハイパーパラメータ設定を試すことが重要です。
##### ReLU (Rectified Linear Unit)
ReLUは最も広く使われる活性化関数の一つで、非線形性を導入することでモデルの表現力を高めます。
ReLUは入力が0より大きい場合はその入力をそのまま出力し、0以下の場合は0を出力します。
ReLUは計算が非常に効率的であり、勾配消失問題を緩和することができますが、学習中に一部のニューロンが「死んで」勾配が0になる問題（死んだReLU問題）があります。
##### Leaky ReLU
Leaky ReLUはReLUの一種で、0以下の入力に対しても微小な勾配を持つことで、死んだReLU問題を緩和します。
##### Sigmoid: 
Sigmoid関数は0から1の間の値を出力するため、確率として解釈するのに便利です。
しかし、Sigmoid関数は出力の範囲が限定されており、極端な入力値では勾配がほぼ0になり、勾配消失問題を引き起こす可能性があります。
##### Tanh (Hyperbolic Tangent)
Tanh関数はSigmoid関数のようにS字形の曲線を描きますが、出力範囲が-1から1であるため、より広範な出力を許容します。
しかし、これもまた極端な入力値では勾配消失問題を引き起こす可能性があります。
##### GELU (Gaussian Error Linear Unit)
GELUはTransformerモデル、特にBERTやT5などのモデルでよく使用される活性化関数です。
GELUは入力の正負に応じて異なる勾配を持つため、モデルが複雑なパターンを学習するのを助けます。
#### 重み初期化方法
これらの初期化方法はそれぞれ異なる特性と利点を持ち、その適用は特定のタスクとモデルアーキテクチャに依存します。
また、初期化方法の選択は、他のハイパーパラメータ（例えば、学習率、ミニバッチサイズ、エポック数など）と密接に関連しています。
適切な重み初期化方法を選択することで、モデルの学習速度と最終的な性能を大幅に改善することができます。
それぞれの初期化方法には特定の使用時の注意があります。
たとえば、Zero Initializationは通常避けるべきであり、Random Initializationは適切なスケールが必要です。
Xavier/Glorot InitializationやHe Initializationは特定の活性化関数と一緒に使用することが推奨されます。
Orthogonal InitializationはRNNに特に有用ですが、全てのネットワーク構造で使用可能なわけではありません。
これらの初期化方法を適切に使用することで、学習の収束を速め、モデルの性能を向上させることができます。
各初期化方法がどのように動作し、どのタイプのネットワークやタスクに最適かを理解することは、成功する深層学習モデルを設計するための重要なスキルです。
##### Zero Initialization
これは最も単純な初期化方法で、すべての重みをゼロに設定します。
しかし、これは通常は避けるべきであり、すべてのニューロンが同じ出力を生成し、学習がうまく進行しない可能性があります。
##### Random Initialization
重みを小さなランダムな値で初期化します。これにより、各ニューロンが独立して学習を開始できます。しかし、重みが非常に大きいか小さい場合、学習中に勾配消失または爆発する可能性があります。
##### Xavier/Glorot Initialization
XavierまたはGlorotの初期化方法は、重みをランダムに初期化するための一般的な方法で、各ニューロンの入力と出力の数に基づいて適切なスケールを計算します。
これは、特にSigmoidやTanhなどの活性化関数と一緒に使用すると効果的です。
##### He Initialization
He初期化はReLUとその派生形（Leaky ReLU、PReLUなど）の活性化関数と一緒に使用するための初期化方法です。
これはXavierの初期化方法と似ていますが、出力の数ではなく入力の数に基づいてスケールを計算します。
##### Orthogonal Initialization
Orthogonal初期化は、重み行列が正規直交行列であるように重みを初期化します。
これは、特に再帰型ニューラルネットワーク（RNN）で有用です。
#### 正則化パラメータ
下記の正則化パラメータとテクニックはそれぞれ異なる特性と利点を持ち、その適用は特定のタスクとモデルアーキテクチャに依存します。
また、正則化パラメータの選択と調整は、他のハイパーパラメータ（例えば、学習率、ミニバッチサイズ、エポック数など）と密接に関連しています。
適切な正則化パラメータを選択し、調整することで、モデルの学習速度と最終的な性能を大幅に改善することができます。
##### L1正則化
L1正則化は、重みの絶対値の合計（L1ノルム）に比例するコストを損失関数に追加します。
これにより、モデルの重みがスパース（つまり、多くの重みがゼロ）になる傾向があります。L1正則化パラメータ（通常λと表記）は、この正則化項の強度を制御します。
##### L2正則化
L2正則化（または重み減衰）は、重みの二乗の合計（L2ノルム）に比例するコストを損失関数に追加します。
これにより、モデルの重みが小さくなる傾向があります。
L2正則化パラメータ（通常λと表記）は、この正則化項の強度を制御します。
##### ドロップアウト
ドロップアウトは、学習中にランダムにニューロンを「ドロップアウト」（つまり、一時的に無効化）することでモデルを正則化するテクニックです。
ドロップアウト率（通常pと表記）は、各学習ステップでドロップアウトするニューロンの割合を制御します。
##### 早期停止
早期停止は、検証データのパフォーマンスが改善しなくなった時点で学習を停止するテクニックです。これにより、モデルの過学習を防ぐことができます。
#### 損失関数
損失関数の選択は、タスク、モデル、データセットに依存します。
例えば、クラスの不均衡が存在する場合、クロスエントロピー損失はクラスの不均衡を増大させる可能性があるため、適切な手法（例えば、重み付け）を用いて調整する必要があります。
ラベルスムージングは、過学習を防ぐ効果があるとされていますが、スムージングの程度（つまり、真のラベルの確信度をどれだけ下げるか）はハイパーパラメータとして調整する必要があります。
損失関数はモデルの学習をどのようにガイドするかを決定します。
したがって、タスクの目的と一致する損失関数を選択することが重要です。
##### クロスエントロピー損失（Cross Entropy Loss）
クロスエントロピー損失は、分類問題やシーケンス生成問題において一般的に使用される損失関数です。
T5などのトランスフォーマーモデルでは、各タイムステップ（単語）での出力分布と、真の単語のワンホットエンコーディングとの間のクロスエントロピーを計算します。
注意点として、この損失関数は出力が確率分布（つまり、出力の合計が1になる）であることを前提としています。
##### ラベルスムージング（Label Smoothing）
ラベルスムージングは、クロスエントロピー損失に対する改良版です。
これは、モデルが一部のクラス（または単語）に対して過信するのを防ぐために、真のラベルの確信度を少し下げるというアイデアに基づいています。
具体的には、各ラベルの確率を少し"スムージング"し、真のラベルだけでなく他のラベルにも一部の確率を割り当てます。
#### 学習率スケジューリング
学習率スケジューリングは、モデルの学習過程で学習率をどのように変更するかを決定する方法で下記の注意すべき点があります。

- ハイパーパラメータの選択
    - 各スケジューリング戦略は異なるハイパーパラメータを必要とします（例えば、ステップデカイの場合はステップ数と減衰率、ウォームアップの場合はウォームアップのステップ数など）。
    これらのハイパーパラメータは学習の性能に大きな影響を及ぼすため、適切に選択することが重要です。
- 学習率の初期値
    - 学習率スケジューリングは初期の学習率から始まります。この初期値は適切に選ばれるべきで、あまりに小さすぎると学習が遅くなり、大きすぎると学習が不安定になる可能性があります。
    一般的には、初期値を0.01や0.001などの比較的大きな値から始め、そこからスケジューリング戦略に従って学習率を減少させることが推奨されます​。
- データとモデルの適応性
    - 最適な学習率スケジューリング戦略は、使用するデータセットやモデルにより異なる場合があります。
    そのため、特定のタスクやモデルに最適なスケジューリング戦略を見つけるためには、異なる戦略を試すことが重要です​​。
- 最適化アルゴリズムとの相互作用
    - 一部の最適化アルゴリズム（例えばAdam）は、内部的に学習率を調整する機能を持つため、これらの最適化アルゴリズムと学習率スケジューリング戦略との間には相互作用が存在します。
    このため、最適化アルゴリズムとスケジューリング戦略を同時に選択する際には注意が必要です​​。
##### ステップ減衰
あらかじめ定義されたステップ数（エポック数）ごとに学習率を一定の割合で減少させます。
この方法では、学習率の減少が急激に行われ、局所的な最適解から抜け出すための「衝撃」を与えることが期待されます。
##### 指数的減衰
学習の進行と共に学習率を指数関数的に減少させます。
これは、学習が進むにつれて学習率が必要となる更新量が減少するという直感に基づいています。
##### 逆時間減衰
学習率を学習ステップの逆数の関数として減少させます。
これは、学習が進むにつれて更新の頻度を減らすというアイデアに基づいています。
##### 余弦退行（Cosine Annealing）
学習率を学習の進行と共にコサイン関数に基づいて減少させます。
この手法では、学習率が初期と終末で低く、中間で高くなるため、学習の初期と終末で探索を強化し、中間では局所的な最適解に収束することを促します。
##### ウォームアップ付きスケジューリング
学習の初期段階では学習率を増加させ（ウォームアップ）、一定のステップ数後には他のスケジューリング戦略（例えば、ステップデカイやコサインアニーリング）に従って学習率を減少させます。
ウォームアップは、学習の初期段階での不安定な振る舞いを防ぐのに役立ちます
### 評価指標
#### Accuracy (精度)
精度は、全体のデータセットに対してモデルが正しく予測したインスタンスの割合を示します。
具体的には、精度は「正しく予測されたインスタンス数」を「全体のインスタンス数」で割ったものです。
しかし、クラスの不均衡が存在する場合（つまり、一部のクラスのインスタンスが他のクラスのインスタンスよりもはるかに多い場合）、精度は誤解を招くことがあります。
なぜなら、多数派のクラスを正しく予測するだけで高い精度が得られるからです。
#### Precision (適合率)
適合率は、モデルが陽性と予測したインスタンスのうち、実際に陽性だったインスタンスの割合を示します。
適合率は「真陽性」（正しく陽性と予測されたもの）を「真陽性＋偽陽性」（陽性と予測されたものの全体）で割ったものです。
適合率は偽陽性（陰性を誤って陽性と予測したもの）の数が少ないことを重視します。
#### Recall (再現率)
再現率は、実際の陽性インスタンスのうち、モデルが陽性と予測したインスタンスの割合を示します。
再現率は「真陽性」を「真陽性＋偽陰性」（陽性と予測すべきものの全体）で割ったものです。
再現率は偽陰性（陽性を誤って陰性と予測したもの）の数が少ないことを重視します。
#### F1 Score
F1スコアは、適合率と再現率の調和平均です。
適合率と再現率はトレードオフの関係にあるため（一方を高めると他方が低下する傾向があるため）、これら二つのバランスを示す指標としてF1スコアがよく使用されます。
#### BLEU (Bilingual Evaluation Understudy)
BLEUスコアは機械翻訳の性能を評価するための指標で、生成された翻訳と参照翻訳（人間が作成した正確な翻訳）との一致度を測ります。
特に、n-gram（連続するn個の単語）の一致度を見ます。
しかし、BLEUスコアは語順の問題や、文脈や意味を正確に捉えることができないという問題があります。
#### ROUGE (Recall-Oriented Understudy for Gisting Evaluation)
ROUGEスコアは、自動要約や機械翻訳の評価に使用される指標で、参照要約（または翻訳）と生成された要約（または翻訳）との間の一致度を計算します。
複数のバージョンがあり、例えばROUGE-Nはn-gramの一致度を、ROUGE-Lは最長共通部分列（LCS）の一致度を計算します。
#### METEOR (Metric for Evaluation of Translation with Explicit ORdering)
METEORは、機械翻訳の評価指標で、BLEUよりも高度な評価を提供します。
同義語や語形変化を認識し、語順も考慮します。また、精度と再現率の調和平均を計算します。
#### Perplexity
Perplexityは、言語モデルの性能を評価するための指標で、言語モデルが次に来る単語をどれだけうまく予測できるかを評価します。
低いパープレキシティは良いモデルを示します。しかし、パープレキシティは絶対的な指標ではなく、特定のタスクに対するモデルの性能を直接反映するわけではありません。
#### Word Error Rate (WER)
WERは、音声認識や機械翻訳の評価に使用され、生成されたテキストと参照テキストとの間の単語レベルでの差異を計算します。
WERは挿入、削除、置換などの操作を考慮に入れます。
#### Character Error Rate (CER)
CERは、生成されたテキストと参照テキストとの間の文字レベルでの差異を計算します。
主に手書き認識や音声認識の評価に使われます。WERと同様に、挿入、削除、置換などの操作を考慮に入れます。
### 考え中
- 尤度
- 演繹
- 加重平均
- ソフトマックス関数
- 重み
- バイアス
- 接続の強さ
- 勾配降下法
- 順伝播
- 逆伝播